# üìã Resumen de Cambios - Chatbot Ollama

## üéØ Objetivo
Reemplazar el sistema de chatbot basado en reglas con una soluci√≥n de IA real usando Ollama + llama2.

## üìÇ Archivos Modificados

### Nuevos Archivos Creados

1. **`src/renderer/src/services/ollamaService.js`** (382 l√≠neas)
   - Servicio que se comunica con Ollama
   - Funciones para verificar disponibilidad de Ollama
   - Construcci√≥n de prompts contextualizados con datos del POS
   - Manejo de errores espec√≠ficos (conexi√≥n rechazada, timeout, etc.)
   - Comando especial `debug` para diagnosticar

2. **`CHATBOT_AI_SETUP.md`**
   - Gu√≠a paso a paso de instalaci√≥n de Ollama
   - Ejemplos de preguntas
   - Soluci√≥n de problemas
   - Recursos √∫tiles

3. **`OLLAMA_SETUP.md`**
   - Gu√≠a detallada de instalaci√≥n
   - Configuraci√≥n recomendada seg√∫n capacidad del PC
   - Comandos √∫tiles de Ollama
   - Troubleshooting avanzado

### Archivos Reemplazados

1. **`src/renderer/src/services/chatbotService.js`**
   - **Antes**: 742 l√≠neas de l√≥gica basada en reglas (Dexie local)
   - **Despu√©s**: 16 l√≠neas que importan de ollamaService
   - Mantiene compatibilidad con ChatWindow.jsx

2. **`src/renderer/src/ui/ChatWindow.jsx`**
   - **Agregado**: Verificaci√≥n de estado de Ollama
   - **Agregado**: Indicador visual de conexi√≥n (‚úÖ conectado / ‚ùå no disponible)
   - **Mejorado**: Timeout extendido a 2 minutos (Ollama puede ser lento)
   - **Mejorado**: Manejo de errores con mensajes √∫tiles
   - **Mejorado**: Desabilita input si Ollama no est√° disponible
   - **Agregado**: Advertencia en pantalla de inicio si Ollama no est√° disponible

3. **`package.json`**
   - **Agregado**: `"axios": "^1.6.2"` para comunicarse con Ollama

## üèóÔ∏è Arquitectura

```
ChatWindow.jsx (UI)
    ‚Üì
chatbotService.js (Adaptador)
    ‚Üì
ollamaService.js (L√≥gica)
    ‚Üì
Ollama (http://localhost:11434/api/generate)
    ‚Üì
llama2 (Modelo de IA)
```

## üîÑ Flujo de Operaci√≥n

1. Usuario abre ChatWindow
2. ChatWindow verifica disponibilidad de Ollama (isOllamaAvailable)
3. Usuario escribe pregunta
4. sendMessage() llama a processChatbotQuery()
5. chatbotService.js llama a processQuery() en ollamaService
6. ollamaService construye prompt contextualizado con datos del POS
7. Env√≠a a Ollama v√≠a HTTP
8. Ollama ejecuta llama2 y devuelve respuesta
9. Respuesta se muestra en ChatWindow

## üí° Ventajas del Nuevo Sistema

| Aspecto | Sistema Anterior | Sistema Nuevo |
|--------|-----------------|----------------|
| **Flexibilidad** | Solo preguntas predefinidas | Cualquier pregunta en lenguaje natural |
| **Inteligencia** | Reglas if/else | Red neural real (llama2) |
| **Contexto** | Datos espec√≠ficos buscados | Entiende contexto completo del POS |
| **Privacidad** | Local (Dexie) | Local (Ollama en PC) |
| **Costo** | Gratis (pero limitado) | Gratis (todo local) |
| **Escalabilidad** | Dif√≠cil (agregar reglas) | F√°cil (el modelo es flexible) |
| **Velocidad de respuesta** | Instant√°neo (lookup) | 5-30 segundos (depende del modelo) |

## üì¶ Dependencias Agregadas

- **axios** (^1.6.2): Cliente HTTP para comunicarse con Ollama
- **Ollama**: Aplicaci√≥n externa (descargada por usuario)
- **llama2 o llama3.2**: Modelo de IA (descargado v√≠a Ollama)

## ‚öôÔ∏è Configuraci√≥n Recomendada

### Para PCs Potentes (16GB+ RAM)
```powershell
ollama pull llama2:13b
```

### Para PCs Normales (8GB RAM)
```powershell
ollama pull llama2:7b
```
O dejar el default `llama2`

### Para PCs Limitadas
```powershell
ollama pull orca-mini:3b
```

## üîê Seguridad y Privacidad

- ‚úÖ Todo funciona localmente en la PC del usuario
- ‚úÖ Ning√∫n dato se env√≠a a servidores externos
- ‚úÖ Ollama se ejecuta en `localhost:11434`
- ‚úÖ No requiere API keys o credenciales
- ‚úÖ Totalmente offline (despu√©s de descargar el modelo)

## üö® Consideraciones

1. **Ollama DEBE estar ejecut√°ndose**
   - Usuario debe dejar `ollama serve` en una terminal
   - Sin Ollama, el chatbot muestra error

2. **Primera respuesta es lenta**
   - Primer request tarda m√°s
   - Siguientes respuestas son m√°s r√°pidas
   - Esto es normal en sistemas locales

3. **Requiere descargas grandes**
   - llama2: ~4GB
   - llama3.2: ~6GB
   - orca-mini:3b: ~2GB

4. **Consumo de recursos**
   - La primera respuesta usa CPU/RAM m√°s intensamente
   - Importante cerrar otras aplicaciones pesadas

## üìä Comparativa de Modelos

| Modelo | Tama√±o | RAM | Velocidad | Inteligencia |
|--------|--------|-----|-----------|--------------|
| orca-mini:3b | 2GB | 4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê |
| llama2:7b | 3.5GB | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |
| llama2 | 4GB | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| llama2:13b | 8GB | 16GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| llama3.2 | Variable | 8GB+ | Variable | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## üõ†Ô∏è Instalaci√≥n para Desarrolladores

1. Ejecutar: `npm install` (instala axios)
2. Descargar Ollama desde ollama.ai
3. Ejecutar: `ollama pull llama2` (o el modelo deseado)
4. Ejecutar: `ollama serve` en una terminal
5. Abrir Trendo POS normalmente
6. Chatbot deber√≠a estar disponible

## ‚úÖ Testing

Para probar:
1. Abrir DevTools (F12)
2. Ir a Console
3. Escribir en el chatbot: "debug"
4. Verificar que muestre datos de ventas/productos
5. Escribir una pregunta normal: "¬øCu√°les son mis ingresos?"
6. Verificar que Ollama responda (puede tardar 5-30 seg)

## üìù Notas

- El sistema mantiene **compatibilidad total** con la UI existente
- Los estilos de ChatWindow siguen siendo los mismos
- Las preguntas sugeridas se mantienen como ejemplos
- La integraci√≥n es limpia y sin efectos secundarios

## üéì Aprendizaje

Este cambio demuestra:
- Migraci√≥n de sistema basado en reglas a IA
- Integraci√≥n de modelos locales (Ollama)
- Construcci√≥n de prompts contextual izados
- Manejo de llamadas HTTP asincr√≥nicas
- Error handling para sistemas externos

---

**Fecha**: Diciembre 2025
**Cambio**: Sistema Chatbot Completo
**Estado**: ‚úÖ Listo para Producci√≥n
